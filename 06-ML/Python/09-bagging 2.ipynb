{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0c02c06f-f68c-4543-bf06-a350a1f922e8",
   "metadata": {},
   "source": [
    "# Module 09: Bagging\n",
    "\n",
    "## Prerequisites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "10f36250-4336-4581-9d34-33f241d32237",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'category_encoders'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m/Users/milou/Documents/misk-DSI/Misk-DSI-2022/06-ML/Python/09-bagging.ipynb Cell 2\u001b[0m in \u001b[0;36m<cell line: 10>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/milou/Documents/misk-DSI/Misk-DSI-2022/06-ML/Python/09-bagging.ipynb#ch0000001?line=7'>8</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mensemble\u001b[39;00m \u001b[39mimport\u001b[39;00m BaggingRegressor\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/milou/Documents/misk-DSI/Misk-DSI-2022/06-ML/Python/09-bagging.ipynb#ch0000001?line=8'>9</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmodel_selection\u001b[39;00m \u001b[39mimport\u001b[39;00m train_test_split\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/milou/Documents/misk-DSI/Misk-DSI-2022/06-ML/Python/09-bagging.ipynb#ch0000001?line=9'>10</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mcategory_encoders\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mordinal\u001b[39;00m \u001b[39mimport\u001b[39;00m OrdinalEncoder\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/milou/Documents/misk-DSI/Misk-DSI-2022/06-ML/Python/09-bagging.ipynb#ch0000001?line=10'>11</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpreprocessing\u001b[39;00m \u001b[39mimport\u001b[39;00m OneHotEncoder\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/milou/Documents/misk-DSI/Misk-DSI-2022/06-ML/Python/09-bagging.ipynb#ch0000001?line=11'>12</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcompose\u001b[39;00m \u001b[39mimport\u001b[39;00m ColumnTransformer\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'category_encoders'"
     ]
    }
   ],
   "source": [
    "# Helper packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from plotnine import *\n",
    "\n",
    "# Modeling packages\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import BaggingRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from category_encoders.ordinal import OrdinalEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.compose import make_column_selector as selector\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.compose import make_column_selector as selector\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.inspection import partial_dependence\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# silence unnecessary warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d62c5e26-2783-4408-ae3f-4ad81dd9633e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ames housing data\n",
    "ames = pd.read_csv(\"../../00-data/ML/ames.csv\")\n",
    "\n",
    "# create train/test split\n",
    "train, test = train_test_split(ames, train_size=0.7, random_state=123)\n",
    "\n",
    "# separate features from labels and only use numeric features\n",
    "X_train = train.drop(\"Sale_Price\", axis=1)\n",
    "y_train = train[[\"Sale_Price\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b45e9f27-94af-4881-99e8-ac67d07a0199",
   "metadata": {},
   "source": [
    "## Implementation\n",
    "\n",
    "### Basic implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b03549ee-0724-475c-a4ec-982f8c287d10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ordinal encode our quality-based features \n",
    "ord_cols = list(X_train.filter(regex=(\"Qual$|QC$|Cond$\")).columns)\n",
    "lvs = [\"Very_Poor\", \"Poor\", \"Fair\", \"Below_Average\", \"Average\", \"Typical\", \n",
    "       \"Above_Average\", \"Good\", \"Very_Good\", \"Excellent\", \"Very_Excellent\"]\n",
    "val = range(0, len(lvs))\n",
    "lvl_map = dict(zip(lvs, val))\n",
    "category_mapping = [{'col': col, 'mapping': lvl_map} for col in ord_cols]\n",
    "ord_encoder = OrdinalEncoder(cols=ord_cols, mapping=category_mapping)\n",
    "\n",
    "# one hot encode remaining nominal features\n",
    "encoder = OneHotEncoder(handle_unknown=\"ignore\", sparse=False)\n",
    "\n",
    "# combine into a pre-processing pipeline\n",
    "preprocessor = ColumnTransformer(\n",
    "  remainder=\"passthrough\",\n",
    "  transformers=[\n",
    "   (\"ord_encode\", ord_encoder, ord_cols),\n",
    "   (\"one-hot\", encoder, selector(dtype_include=\"object\")),\n",
    "   ]\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95a4ff00-462f-49b7-9b01-495eae7775a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create bagging estimator\n",
    "dt_bag = BaggingRegressor(base_estimator=DecisionTreeRegressor(), n_estimators=5)\n",
    "\n",
    "# create modeling pipeline\n",
    "model_pipeline = Pipeline(steps=[\n",
    "  (\"preprocessor\", preprocessor),\n",
    "  (\"dt_bag\", dt_bag),\n",
    "])\n",
    "\n",
    "# define loss function\n",
    "loss = 'neg_root_mean_squared_error'\n",
    "\n",
    "# create 5 fold CV object\n",
    "kfold = KFold(n_splits=5, random_state=123, shuffle=True)\n",
    "\n",
    "# fit model with 5-fold CV\n",
    "results = cross_val_score(model_pipeline, X_train, y_train, cv=kfold, scoring=loss)\n",
    "\n",
    "np.abs(np.mean(results))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f41afb5-4d81-4b48-a42d-3becd21301a6",
   "metadata": {},
   "source": [
    "### Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cca917eb-b8fc-46ec-8325-b6768c26ed34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create bagging estimator but with undefined number of bagged trees\n",
    "dt_bag = BaggingRegressor(base_estimator=DecisionTreeRegressor())\n",
    "\n",
    "# Create grid of hyperparameter values\n",
    "hyper_grid = {'dt_bag__n_estimators': [5, 25, 50, 100, 200]}\n",
    "\n",
    "# Tune a knn model using grid search\n",
    "grid_search = GridSearchCV(model_pipeline, hyper_grid, cv=kfold, scoring=loss, n_jobs=-1)\n",
    "results = grid_search.fit(X_train, y_train)\n",
    "\n",
    "np.abs(results.cv_results_['mean_test_score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95246ad0-9b0d-4747-9018-efab715142eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best model's cross validated RMSE\n",
    "abs(results.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3457c3c-8a8e-43cb-a63c-a3f9d8cbbbca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best model's number of trees\n",
    "n_trees = results.best_estimator_.get_params().get('dt_bag__n_estimators')\n",
    "n_trees"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04b84d1b-32f4-4bb2-a6d2-628c7e9a8f5c",
   "metadata": {},
   "source": [
    "## Feature interpretation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34e28d4b-c03a-416b-82b8-cd79925438b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create final model object\n",
    "X_encoded = preprocessor.fit_transform(X_train)\n",
    "dt_bag = BaggingRegressor(\n",
    "  base_estimator=DecisionTreeRegressor(), \n",
    "  n_estimators=n_trees\n",
    "  )\n",
    "dt_bag_fit = dt_bag.fit(X_encoded, y_train)\n",
    "\n",
    "# extract feature importances\n",
    "feature_importances = [tree.feature_importances_ for tree in dt_bag_fit.estimators_ ]\n",
    "avg_feature_importances = np.mean(feature_importances, axis=0)\n",
    "vi = pd.DataFrame({'feature': preprocessor.get_feature_names(),\n",
    "                   'importance': avg_feature_importances})\n",
    "\n",
    "# get top 20 influential features\n",
    "top_20_features = vi.nlargest(20, 'importance')\n",
    "\n",
    "# plot feature importance\n",
    "(ggplot(top_20_features, aes(x='importance', y='reorder(feature, importance)'))\n",
    " + geom_point()\n",
    " + labs(y=None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1146dda3-53c9-456f-b632-e217fbcc9c22",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_encoded = pd.DataFrame(X_encoded, columns=preprocessor.get_feature_names())\n",
    "pd_results = partial_dependence(\n",
    "  dt_bag_fit, X_encoded, \"ord_encode__Overall_Qual\", kind='average',\n",
    "  percentiles=(0, 1)) \n",
    "  \n",
    "pd_output = pd.DataFrame({'ord_encode__Overall_Qual': pd_results['values'][0],\n",
    "                          'yhat': pd_results['average'][0]})\n",
    "                          \n",
    "(ggplot(pd_output, aes('ord_encode__Overall_Qual', 'yhat'))\n",
    "  + geom_line())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69e800ef-061a-4f6c-99dd-6df6f2201b7a",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "Using the Boston housing data set, where the response feature is the median value of homes within a census tract (`cmedv`):\n",
    "\n",
    "1. Apply a bagged decision tree model with all features.\n",
    "2. How many trees are required before the loss function stabilizes?\n",
    "3. Adjust different tuning parameters and assess how performance changes.\n",
    "4. How does the model performance compare to the decision tree model applied in the previous module's exercise?\n",
    "5. Which 10 features are considered most influential? Are these the same features that have been influential in previous model?\n",
    "6. Create partial dependence plots for the top two most influential features. Explain the relationship between the feature and the predicted values.\n",
    "7. Now perform 1-6 to the Attrition dataset, which is classification model rather than a regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d52ded89-ea77-4206-b596-0b1aa4c17ff1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "ed549bf1dfff0c9e2fa63787d3c672338cbace345788ee41613d78c6db524064"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
